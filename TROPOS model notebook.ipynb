{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "TROPOS.ipynb",
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Acvmhv4FVc35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade pip\n",
        "# !pip install scikit-multilearn\n",
        "# !pip install visualkeras"
      ],
      "metadata": {
        "id": "1zl9XJFzJR9m",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install netCDF4\n",
        "# !pip install zarr\n",
        "# !pip install xarray\n",
        "# !pip install tensorflow_addons\n",
        "# !pip install h5netcdf\n",
        "# !pip install tensorflow[and-cuda]\n",
        "# !pip install tbparse"
      ],
      "metadata": {
        "id": "rkddJ0981fGd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! gsutil -m cp -r dir gs://tropos_2/limassol /content\n",
        "# !pip install tensorboard pandas\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PgXBb3J7Icos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "IOlKV3BaIrpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLbKQYP9Ju0Y"
      },
      "outputs": [],
      "source": [
        "import netCDF4 as nc\n",
        "import matplotlib.pyplot as plt\n",
        "from google.cloud import storage\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.colors as colors\n",
        "import matplotlib.cbook as cbook\n",
        "import math\n",
        "import zarr\n",
        "import xarray as xr\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "# import tensorflow_addons as tfa\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.backend as K"
      ],
      "metadata": {
        "id": "f8d-uD6lVlCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fsspec\n",
        "import concurrent.futures\n",
        "from tqdm.notebook import tqdm_notebook, trange, tqdm\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import plotly.express as px\n",
        "import matplotlib.colors as colors"
      ],
      "metadata": {
        "id": "49BqcGvRfpu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union, List, Callable\n",
        "import matplotlib.colors as mcolors"
      ],
      "metadata": {
        "id": "r2tlKsXeM2cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializations"
      ],
      "metadata": {
        "id": "Wl9Y_LKygXCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test tf and gpu compatibility"
      ],
      "metadata": {
        "id": "Ji6pUSejgT5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(\"Num GPUs Available: \", len(gpus))\n",
        "if gpus:\n",
        "    try:\n",
        "        # Print details for each GPU\n",
        "        for gpu in gpus:\n",
        "            print(\"Found GPU:\", gpu)\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(\"  Memory growth enabled.\")\n",
        "        # Try a simple GPU operation\n",
        "        print(\"\\nAttempting simple GPU operation...\")\n",
        "        with tf.device('/GPU:0'):\n",
        "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
        "            b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
        "            c = tf.matmul(a, b)\n",
        "        print(\"Simple GPU operation successful. Result tensor:\\n\", c.numpy())\n",
        "    except RuntimeError as e:\n",
        "        print(\"!!! Runtime Error during GPU setup or test:\", e)\n",
        "    except Exception as e:\n",
        "        print(\"!!! An unexpected error occurred:\", e)\n",
        "else:\n",
        "    print(\"!!! TensorFlow cannot find any GPUs.\")\n",
        "    print(\"!!! Ensure you have 'tensorflow' (GPU version), not 'tensorflow-cpu' installed.\")\n",
        "    print(\"!!! Check CUDA/cuDNN installation and compatibility.\")"
      ],
      "metadata": {
        "id": "kvYMg99ygReR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Parameters"
      ],
      "metadata": {
        "id": "wx7mpGDc6C85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_runtime = None # @param\n",
        "if not model_runtime:\n",
        "  model_runtime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "model_run_name = '4_layer_large_filter_no_class0_mean1_group_and_dice_penalty1_data_globalnorm' # @param\n",
        "model_name = f'{model_run_name}_{model_runtime}'"
      ],
      "metadata": {
        "id": "IDm92Q-J6Bqg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "9a8iWl_ahBfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Data"
      ],
      "metadata": {
        "id": "Jb9NaPQnVhz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_data = False # @param\n",
        "use_mask = True # @param\n",
        "dataset_name = 'data_w_mask_global_norm_3' # @param\n",
        "train = False # @param"
      ],
      "metadata": {
        "id": "mGGTEzKp3nL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_filepath = \"limassol\"\n",
        "files_glob = tf.io.gfile.glob(local_filepath + \"/*\")\n",
        "print(len(files_glob))"
      ],
      "metadata": {
        "id": "zdKC_1zSVhMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_nc_file(filename, engine=\"h5netcdf\", *args, **kwargs) -> xr.Dataset:\n",
        "    \"\"\"Load a NetCDF dataset from local file system or cloud bucket.\"\"\"\n",
        "    with fsspec.open(filename, mode=\"rb\") as file:\n",
        "        dataset = xr.load_dataset(file, engine=engine, *args, **kwargs)\n",
        "    return dataset\n",
        "\n",
        "# load_nc_file('limassol/20170107_regridded_data_for_limassol.nc')"
      ],
      "metadata": {
        "id": "bYkak18woJlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset attributes\n",
        "_HEADS = features = [\n",
        "    'polly_bsc_532',\n",
        "    'polly_bsc_1064',\n",
        "    'polly_att_bsc_532',\n",
        "    'polly_att_bsc_1064',\n",
        "    'polly_pardepol_532',\n",
        "    'polly_voldepol_532',\n",
        "    'polly_ang_532_1064',\n",
        "    'model_pressure',\n",
        "    'model_temperature'\n",
        "]\n",
        "\n",
        "_QUALITY_FLAGS = [\n",
        "    'polly_bsc_532_quality_flag',\n",
        "    'polly_bsc_1064_quality_flag',\n",
        "    'polly_voldepol_532_quality_flag',\n",
        "]\n",
        "\n",
        "_CLASS_NAME = 'combined_target_classification'\n",
        "_MODEL_CLASS_NAME = 'polly_target_classification'\n",
        "\n",
        "# Define dataset sizes\n",
        "times_min_dim = 960\n",
        "heights_min_dim = 600\n",
        "\n",
        "num_classes = 12"
      ],
      "metadata": {
        "id": "1ILCBeLeAPuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_files(file, times_min_dim=960, heights_min_dim=600):\n",
        "  # Get file list with data meeting required lengths\n",
        "    ds = load_nc_file(file)\n",
        "    time_len = len(ds.coords['time'].values)\n",
        "    height_len = len(ds.coords['height'].values)\n",
        "    if time_len != times_min_dim:\n",
        "        print(file, time_len, height_len)\n",
        "        return file\n",
        "    if height_len != heights_min_dim:\n",
        "        print(file, time_len, height_len)\n",
        "        return file\n",
        "    else:\n",
        "      return None"
      ],
      "metadata": {
        "id": "Zoa03iardZmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if generate_data:\n",
        "#   files_to_remove = []\n",
        "#   with concurrent.futures.ThreadPoolExecutor(max_workers=64) as executor:\n",
        "#     all_data = executor.map(filter_files, files_glob)\n",
        "#     files_to_remove = list(all_data)\n",
        "#   print(len(files_to_remove))\n",
        "\n",
        "files_to_remove = ['limassol/20171016_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170508_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170303_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170308_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170527_regridded_data_for_limassol.nc',\n",
        "             'limassol/20171019_regridded_data_for_limassol.nc',\n",
        "             'limassol/20180222_regridded_data_for_limassol.nc',\n",
        "             'limassol/20161225_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170413_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170304_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170215_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170514_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170310_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170524_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170906_regridded_data_for_limassol.nc',\n",
        "             'limassol/20180121_regridded_data_for_limassol.nc',\n",
        "             'limassol/20171020_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170401_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170419_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170511_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170522_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170101_regridded_data_for_limassol.nc',\n",
        "             'limassol/20161213_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170320_regridded_data_for_limassol.nc',\n",
        "             'limassol/20180325_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170210_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170225_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170331_regridded_data_for_limassol.nc',\n",
        "             'limassol/20180219_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170213_regridded_data_for_limassol.nc',\n",
        "             'limassol/20170402_regridded_data_for_limassol.nc']"
      ],
      "metadata": {
        "id": "yTi67FErdhYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems that 29 files have less than 960 times. We can either get rid of them or to fill nans for the missing times."
      ],
      "metadata": {
        "id": "Fyf8Lm3Qw6ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove problematic files\n",
        "if generate_data:\n",
        "  for file in files_to_remove:\n",
        "    if file:\n",
        "      files_glob.remove(file)\n",
        "len(files_glob)"
      ],
      "metadata": {
        "id": "JudEStmcJ7L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset parameters\n",
        "_TIMES =  times_min_dim\n",
        "_HEIGHTS = heights_min_dim\n",
        "\n",
        "_N_FEATURES = len(_HEADS)\n",
        "_N_SAMPLES = len(files_glob)\n",
        "\n",
        "_INPUT_SHAPE = (_TIMES, _HEIGHTS, _N_FEATURES*2)\n",
        "_INPUT_SHAPE"
      ],
      "metadata": {
        "id": "aYCjln9wJ-by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Generate input data tensor, shape = [n_samples, times, heights, n_features]:\n",
        "def get_mask(ds, feature_name):\n",
        "    \"\"\"Mask the dataset\"\"\"\n",
        "    if feature_name in ['polly_bsc_532', 'polly_att_bsc_532']:\n",
        "        feature_mask = np.array(ds.variables['polly_bsc_532_quality_flag'][:].values, dtype=bool)\n",
        "    elif feature_name in ['polly_bsc_1064', 'polly_att_bsc_1064']:\n",
        "        feature_mask = np.array(ds.variables['polly_bsc_1064_quality_flag'][:].values, dtype=bool)\n",
        "    elif feature_name == 'polly_voldepol_532':\n",
        "        feature_mask = np.array(ds.variables['polly_voldepol_532_quality_flag'][:].values, dtype=bool)\n",
        "    elif feature_name == 'polly_pardepol_532':\n",
        "        feature_mask = np.array(ds.variables['polly_bsc_532_quality_flag'][:].values, dtype=bool) | np.array(ds.variables['polly_voldepol_532_quality_flag'][:].values, dtype=bool)\n",
        "    elif feature_name == 'polly_ang_532_1064':\n",
        "        feature_mask = np.array(ds.variables['polly_bsc_532_quality_flag'][:].values, dtype=bool) | np.array(ds.variables['polly_bsc_1064_quality_flag'][:].values, dtype=bool)\n",
        "    else:\n",
        "        feature_mask = np.array(np.zeros(ds.variables[feature_name].values.shape), dtype=bool)\n",
        "    return feature_mask"
      ],
      "metadata": {
        "id": "XoFfo5dIKJJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_feature_matrix(\n",
        "    files_list,\n",
        "    use_mask: bool = False\n",
        "    ):\n",
        "    \"\"\"Functions that creates the feature matrix.\"\"\"\n",
        "    input_data = np.zeros([_N_SAMPLES, _TIMES, _HEIGHTS, _N_FEATURES])\n",
        "    label_data = np.zeros([_N_SAMPLES, _TIMES, _HEIGHTS, 1])\n",
        "    curr_model_label_data = np.zeros([_N_SAMPLES, _TIMES, _HEIGHTS, 1])\n",
        "\n",
        "    cnt_file = 0\n",
        "    for i in trange(len(files_list)):\n",
        "      try:\n",
        "          file = files_list[i]\n",
        "          ds = load_nc_file(file)\n",
        "          cnt_features = 0\n",
        "          for i in _HEADS:\n",
        "              data = ds.variables[i][:].values\n",
        "              if use_mask:\n",
        "                  mask = get_mask(ds, i)\n",
        "                  if mask.shape != data.shape:\n",
        "                    print('Shape mismatch for: ', file, i, 'Shapes: ', mask.shape, data.shape)\n",
        "                  data = np.where(~mask, data, np.nan)\n",
        "                  if np.all(np.isnan(data)):\n",
        "                    print('All values are nan for: ', file, i)\n",
        "              #--------------------------------------------------\n",
        "              clipped_data = data[:_TIMES,:_HEIGHTS]\n",
        "              input_data[cnt_file,:,:,cnt_features] = clipped_data\n",
        "              cnt_features += 1\n",
        "      except:\n",
        "          print(f'{file}, {i}')\n",
        "          print(input_data.shape)\n",
        "          continue\n",
        "      class_values = ds.variables[_CLASS_NAME].values\n",
        "      saturated_class = class_values[:_TIMES,:_HEIGHTS]\n",
        "      saturated_class = saturated_class.reshape(_TIMES,_HEIGHTS,1)\n",
        "      label_data[cnt_file,:,:] = saturated_class\n",
        "\n",
        "      curr_model_class_values = ds.variables[_MODEL_CLASS_NAME].values\n",
        "      curr_model_saturated_class = curr_model_class_values[:_TIMES,:_HEIGHTS]\n",
        "      curr_model_saturated_class = curr_model_saturated_class.reshape(_TIMES,_HEIGHTS,1)\n",
        "      curr_model_label_data[cnt_file,:,:] = curr_model_saturated_class\n",
        "\n",
        "      cnt_file += 1\n",
        "    return input_data, label_data, curr_model_label_data"
      ],
      "metadata": {
        "id": "LlhOakXoKPt_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if generate_data:\n",
        "  input_data, label_data, curr_model_label_data = create_feature_matrix(files_list=files_glob, use_mask=use_mask)\n",
        "  np.save(f'dataset_for_training/{dataset_name}_input.npy', input_data)\n",
        "  np.save(f'dataset_for_training/{dataset_name}_label.npy', label_data)\n",
        "  np.save(f'dataset_for_training/{dataset_name}_curr_model_label.npy', curr_model_label_data)\n",
        "else:\n",
        "  input_data = np.load(f'dataset_for_training/{dataset_name}_input.npy')\n",
        "  label_data = np.load(f'dataset_for_training/{dataset_name}_label.npy')\n",
        "  curr_model_label_data = np.load(f'dataset_for_training/{dataset_name}_curr_model_label.npy')"
      ],
      "metadata": {
        "id": "0hYCuvYvFKP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'label data: {label_data.shape}')\n",
        "print(f'input data: {input_data.shape}')\n"
      ],
      "metadata": {
        "id": "7uaQS2by8aa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare datasets"
      ],
      "metadata": {
        "id": "if1EV4VbSwfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "SEED = 42  # Set a seed for reproducible random shuffling\n",
        "TEST_RATIO = 0.2 # Proportion of the *total* data for the test set\n",
        "VALID_RATIO = 0.1 # Proportion of the *total* data for the validation set"
      ],
      "metadata": {
        "id": "u8phLV8kTFEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Splitting Setup ---\n",
        "n_samples = input_data.shape[0]\n",
        "\n",
        "print(f'\\nTotal number of samples: {n_samples}')\n",
        "train_ratio = 1.0 - TEST_RATIO - VALID_RATIO\n",
        "print(f'Splitting Ratios -> Train: {train_ratio:.2f}, Validation: {VALID_RATIO:.2f}, Test: {TEST_RATIO:.2f}')\n",
        "\n",
        "# Calculate number of samples for each set based on total samples\n",
        "# Use integer casting after multiplication to get sample counts\n",
        "n_test = int(TEST_RATIO * n_samples)\n",
        "n_valid = int(VALID_RATIO * n_samples)\n",
        "# Train gets the remainder to avoid rounding errors losing samples\n",
        "n_train = n_samples - n_test - n_valid\n",
        "\n",
        "print(f'Calculated Samples -> Train: {n_train}, Validation: {n_valid}, Test: {n_test}')\n",
        "print(f'Total allocated: {n_train + n_valid + n_test} (should equal {n_samples})')\n"
      ],
      "metadata": {
        "id": "494MFW43TFIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Random Splitting using Shuffled Indices ---\n",
        "print(f\"\\nGenerating and shuffling indices with seed {SEED}...\")\n",
        "np.random.seed(SEED)\n",
        "indices = np.arange(n_samples)\n",
        "np.random.shuffle(indices)\n",
        "print(\"Indices shuffled.\")"
      ],
      "metadata": {
        "id": "QY7l7svPT5-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine split points in the shuffled indices array\n",
        "test_indices = indices[:n_test]\n",
        "valid_indices = indices[n_test : n_test + n_valid]\n",
        "train_indices = indices[n_test + n_valid :] # The rest go to training\n",
        "\n",
        "print(f\"Indices used -> Test: {len(test_indices)}, Validation: {len(valid_indices)}, Train: {len(train_indices)}\")\n",
        "\n",
        "# Create the datasets using the shuffled indices\n",
        "# This ensures that X, Y, and Y_curr_model samples stay paired correctly.\n",
        "print(\"Creating data splits using shuffled indices...\")\n",
        "x_train = input_data[train_indices]\n",
        "y_train_int = label_data[train_indices] # Keep original integer labels for now\n",
        "\n",
        "x_valid = input_data[valid_indices]\n",
        "y_valid_int = label_data[valid_indices]\n",
        "\n",
        "x_test = input_data[test_indices]\n",
        "y_test_int = label_data[test_indices]\n",
        "# Only create the test split for the 'current model' labels\n",
        "y_curr_model_test_int = curr_model_label_data[test_indices]\n",
        "print(\"Data splits created.\")\n"
      ],
      "metadata": {
        "id": "ZXjRxhMLT9hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def class_variance(y_int, set='Train'):\n",
        "  n_samples = y_int.shape[0]\n",
        "  y_int_samples_squeezed = np.squeeze(y_int, axis=-1)\n",
        "\n",
        "  all_unique_classes_per_sample = []\n",
        "  for i in range(n_samples):\n",
        "      sample_labels = y_int_samples_squeezed[i]\n",
        "      unique_in_sample = np.unique(sample_labels.flatten())\n",
        "      all_unique_classes_per_sample.extend(unique_in_sample)\n",
        "\n",
        "  fig = px.histogram(all_unique_classes_per_sample)\n",
        "  fig.show()\n",
        "\n",
        "  class_presence_counts = np.bincount(all_unique_classes_per_sample, minlength=num_classes)\n",
        "  return class_presence_counts/n_samples\n"
      ],
      "metadata": {
        "id": "DLoroBuAdqiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_distribution(y_int):\n",
        "    # Ensure the input is a numpy array\n",
        "    y_int = np.array(y_int)\n",
        "\n",
        "    # Squeeze the last dimension if it's 1 (e.g., for channel)\n",
        "    if y_int.shape[-1] == 1:\n",
        "        y_int_squeezed = np.squeeze(y_int, axis=-1)\n",
        "    else:\n",
        "        y_int_squeezed = y_int\n",
        "\n",
        "    all_unique_classes_per_sample = []\n",
        "    # Iterate over each sample (image/mask) in the dataset\n",
        "    for i in range(y_int_squeezed.shape[0]):\n",
        "        sample_labels = y_int_squeezed[i]\n",
        "        # Find the unique classes present in the current sample\n",
        "        unique_in_sample = np.unique(sample_labels.flatten())\n",
        "        all_unique_classes_per_sample.extend(unique_in_sample)\n",
        "\n",
        "    return all_unique_classes_per_sample\n",
        "\n",
        "def plot_class_distribution_density(y_train, y_val, y_test, num_classes):\n",
        "    \"\"\"\n",
        "    Calculates and plots the class distribution as overlaid density plots\n",
        "    for train, validation, and test sets in a single figure.\n",
        "\n",
        "    Args:\n",
        "        y_train (np.ndarray): Training dataset labels.\n",
        "        y_val (np.ndarray): Validation dataset labels.\n",
        "        y_test (np.ndarray): Test dataset labels.\n",
        "        num_classes (int): The total number of classes to set the x-axis range.\n",
        "    \"\"\"\n",
        "    # Create a single figure and axis for the density plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 7), sharey=True)\n",
        "\n",
        "    datasets = {\n",
        "        'Train Set': y_train,\n",
        "        'Validation Set': y_val,\n",
        "        'Test Set': y_test\n",
        "    }\n",
        "\n",
        "    # Use seaborn's color palette for distinct colors\n",
        "    colors = sns.color_palette('deep', n_colors=len(datasets))\n",
        "    # colors = [\"#3498db\", \"#e74c3c\", \"#2ecc71\"]\n",
        "\n",
        "    # Loop through each dataset to create and add its density plot\n",
        "    pd_dists = []\n",
        "    for i, (name, data) in enumerate(datasets.items()):\n",
        "        # Get the class distribution data using the helper function\n",
        "        dist_data_pd = pd.DataFrame({'values':get_class_distribution(data)})\n",
        "        dist_data_pd['Dataset'] = name\n",
        "        pd_dists.append(dist_data_pd)\n",
        "\n",
        "    dist_data = pd.concat(pd_dists)\n",
        "    sns.kdeplot(data=dist_data, x='values', hue='Dataset', ax=ax,\n",
        "                 common_norm=False,\n",
        "                 alpha=0.8,\n",
        "                 linewidth=2,\n",
        "                #  discrete=True,\n",
        "                #  stat='percent',\n",
        "                #  multiple=\"layer\",\n",
        "                #  shrink=.9,\n",
        "                 cut=0,\n",
        "                 palette=colors)\n",
        "\n",
        "\n",
        "    # --- Customize the plot ---\n",
        "    ax.set_xlabel(\"Class ID\", fontsize=12)\n",
        "    ax.set_ylabel(\"Density\", fontsize=12)\n",
        "    ax.set_xticks(range(num_classes)) # Ensure ticks are at integer class labels\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.9)\n",
        "\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_class_distribution_density(y_train_int, y_valid_int, y_test_int, 12)"
      ],
      "metadata": {
        "id": "QTt1LI3btPYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_class_distribution_density(y_train, y_val, y_test, num_classes):\n",
        "    \"\"\"\n",
        "    Calculates and plots the class distribution as overlaid density plots\n",
        "    for train, validation, and test sets in a single figure.\n",
        "\n",
        "    Args:\n",
        "        y_train (np.ndarray): Training dataset labels.\n",
        "        y_val (np.ndarray): Validation dataset labels.\n",
        "        y_test (np.ndarray): Test dataset labels.\n",
        "        num_classes (int): The total number of classes to set the x-axis range.\n",
        "    \"\"\"\n",
        "    # Create a single figure and axis for the density plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 7), sharey=True)\n",
        "\n",
        "    datasets = {\n",
        "        'Train Set': y_train,\n",
        "        'Validation Set': y_val,\n",
        "        'Test Set': y_test\n",
        "    }\n",
        "\n",
        "    # Use seaborn's color palette for distinct colors\n",
        "    colors = sns.color_palette('deep', n_colors=len(datasets))\n",
        "    # colors = [\"#3498db\", \"#e74c3c\", \"#2ecc71\"]\n",
        "\n",
        "    # Loop through each dataset to create and add its density plot\n",
        "    pd_dists = []\n",
        "    for i, (name, data) in enumerate(datasets.items()):\n",
        "        # 2. Calculate the histogram density and the bin edges\n",
        "        density, edges = np.histogram(get_class_distribution(data), bins=num_classes, density=True)\n",
        "\n",
        "        # 3. Calculate the center of each bin for the x-coordinate\n",
        "        centers = (edges[:-1] + edges[1:]) / 2\n",
        "\n",
        "        # 4. Plot the density as a point at the center of each bin\n",
        "        ax.plot(centers, density,\n",
        "                marker='o',        # Use a circle as the marker\n",
        "                linestyle='-',      # No connecting line between markers\n",
        "                ms=8,              # Set the marker size\n",
        "                label=name,        # Add a label for the legend\n",
        "                color=colors[i])\n",
        "\n",
        "    # --- Customize the plot ---\n",
        "    ax.set_xlabel(\"Class ID\", fontsize=12)\n",
        "    ax.set_ylabel(\"Density\", fontsize=12)\n",
        "    ax.set_xticks(range(num_classes)) # Ensure ticks are at integer class labels\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.9)\n",
        "    ax.legend(title='Dataset') # Add a legend to identify the points\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_class_distribution_density(y_train_int, y_valid_int, y_test_int, 12)"
      ],
      "metadata": {
        "id": "L_SLWBF4OV-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess dataset"
      ],
      "metadata": {
        "id": "sNppc0sfNfa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train data\n",
        "x_train_clipped = x_train.copy()\n",
        "\n",
        "for i in range(_N_FEATURES):\n",
        "  x_train_clipped[...,i] = np.clip(\n",
        "      x_train_clipped[...,i], 0, np.nanpercentile(\n",
        "          x_train_clipped[...,i], 99))\n",
        "\n",
        "RAW_MEANS = np.nanmean(x_train_clipped, axis=(0,1,2))\n",
        "RAW_MEANS = RAW_MEANS[np.newaxis, np.newaxis, np.newaxis, :]"
      ],
      "metadata": {
        "id": "s29QyMsfv3yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_temp = x_test[6,...,6].copy()\n",
        "x_temp = np.clip(x_temp, 0, None)\n",
        "# replace nan\n",
        "x_temp = np.nan_to_num(x_temp, nan=RAW_MEANS[...,6], posinf=None, copy=True)\n",
        "values = np.concatenate([x_temp.flatten(), np.log1p(x_temp.flatten())])\n",
        "# Create a corresponding label for each value\n",
        "labels = ['Original'] * len(x_temp.flatten()) + ['Log-Transformed'] * len(np.log1p(x_temp.flatten()))\n",
        "# Build the DataFrame\n",
        "combined_df = pd.DataFrame({'Pixel Value': values, 'Data Type': labels})\n",
        "\n",
        "\n",
        "# 3. Create the plot using hue\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
        "fig.subplots_adjust(hspace=0.05)  # adjust space between Axes\n",
        "fig.set_size_inches(8, 6)\n",
        "sns.kdeplot(data=combined_df, x='Pixel Value', hue='Data Type', fill=True, common_norm=False, ax=ax1)\n",
        "sns.kdeplot(data=combined_df, x='Pixel Value', hue='Data Type', fill=True, common_norm=False, ax=ax2, legend=False)\n",
        "\n",
        "ax1.set_ylim(15, 18)  # outliers only\n",
        "ax2.set_ylim(0, 0.4)  # most of the data\n",
        "\n",
        "# hide the spines between ax and ax2\n",
        "ax1.spines.bottom.set_visible(False)\n",
        "ax2.spines.top.set_visible(False)\n",
        "ax1.xaxis.tick_top()\n",
        "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
        "ax2.xaxis.tick_bottom()\n",
        "\n",
        "d = .5  # proportion of vertical to horizontal extent of the slanted line\n",
        "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12,\n",
        "              linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
        "ax1.plot([0, 1], [0, 0], transform=ax1.transAxes, **kwargs)\n",
        "ax2.plot([0, 1], [1, 1], transform=ax2.transAxes, **kwargs)\n",
        "plt.xlim((-0.5,7))\n",
        "\n",
        "# 4. Add titles and labels\n",
        "ax1.set_ylabel('')\n",
        "ax2.set_ylabel('')\n",
        "fig.supylabel('Density')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RSBm6-jywYNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data, means=None, stds=None, training=True, with_log=True):\n",
        "  # clip negatives\n",
        "  data = np.clip(data, 0, None)\n",
        "  # get nan feature\n",
        "  x_nans = np.isnan(data).astype(np.int16)\n",
        "  # replace nan\n",
        "  data = np.nan_to_num(data, nan=RAW_MEANS, posinf=None, copy=True)\n",
        "  # log transform\n",
        "  if with_log:\n",
        "    data = np.log1p(data)\n",
        "  # calc means and stds\n",
        "  if training:\n",
        "    means = np.nanmean(data, axis=(0,1,2))\n",
        "    means = means[np.newaxis, np.newaxis, np.newaxis, :]\n",
        "    stds = np.nanstd(data, axis=(0,1,2))\n",
        "    stds = stds[np.newaxis, np.newaxis, np.newaxis, :]\n",
        "  # Standardize\n",
        "  data = (data-means)/stds\n",
        "  # add nan feature\n",
        "  data = np.concatenate([data, x_nans], axis=3)\n",
        "  return data, means, stds\n",
        "\n",
        "x_train, log_means, log_stds = preprocess(x_train)\n",
        "x_valid, _, _ = preprocess(x_valid, log_means, log_stds, False)\n",
        "x_test, _, _ = preprocess(x_test, log_means, log_stds, False)"
      ],
      "metadata": {
        "id": "4etX7CNDfgiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Deleting original large arrays to free memory...\")\n",
        "del input_data\n",
        "del label_data\n",
        "del curr_model_label_data\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "0bNdx5TUUSw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nOne-hot encoding labels with num_classes={num_classes}...\")\n",
        "# Use float32 for compatibility with most models/losses\n",
        "y_train = tf.keras.utils.to_categorical(y_train_int, num_classes=num_classes).astype(np.float32)\n",
        "y_valid = tf.keras.utils.to_categorical(y_valid_int, num_classes=num_classes).astype(np.float32)\n",
        "y_test = tf.keras.utils.to_categorical(y_test_int, num_classes=num_classes).astype(np.float32)\n",
        "y_curr_model_test = tf.keras.utils.to_categorical(y_curr_model_test_int, num_classes=num_classes).astype(np.float32)\n",
        "print(\"One-hot encoding complete.\")"
      ],
      "metadata": {
        "id": "NYlr6C7hUVDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('x_train:',np.shape(x_train))\n",
        "print('y_train:',np.shape(y_train))\n",
        "\n",
        "print('x_test:',np.shape(x_test))\n",
        "print('x_test:',np.shape(x_test))\n",
        "\n",
        "print('x_valid:',np.shape(x_valid))\n",
        "print('y_valid:',np.shape(y_valid))\n",
        "\n",
        "print('y_curr_model_test:',np.shape(y_curr_model_test))"
      ],
      "metadata": {
        "id": "phpPc6hZOksY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize dataset\n",
        "\n",
        "# Choose which image to visualize\n",
        "y_valid_img = y_valid[2]\n",
        "# -----\n",
        "\n",
        "for i in range(_TIMES):\n",
        "    for j in range(_HEIGHTS):\n",
        "        indices = np.where(y_valid_img[i, j, :])[0]\n",
        "        if len(indices) > 1:\n",
        "            active_index = np.random.choice(indices)\n",
        "            y_valid_img[i, j, :] = 0\n",
        "            y_valid_img[i, j, active_index] = 1\n",
        "        elif len(indices) == 0:\n",
        "            y_valid_img[i, j, 0] = 1\n",
        "\n",
        "# Define colors for your 12 classes\n",
        "colors = plt.cm.get_cmap('viridis', num_classes)\n",
        "class_colors = colors(np.linspace(0, 1, num_classes))\n",
        "\n",
        "# Create an RGB image where each class is mapped to a color\n",
        "colored_image = np.zeros((960, 600, 3), dtype=np.float32)\n",
        "\n",
        "for i in range(12):\n",
        "    mask = y_valid_img[:, :, i] == 1\n",
        "    colored_image[mask] = class_colors[i, :3]\n",
        "\n",
        "# Display the colored image using Matplotlib\n",
        "fig, ax = plt.subplots(figsize=(10, 8))  # Create a figure and an axes object\n",
        "image = ax.imshow(colored_image.transpose(1, 0, 2), origin='lower')\n",
        "# ax.set_title(\"Segmentation Visualization\")\n",
        "ax.set_xlabel(\"X-axis\")\n",
        "ax.set_ylabel(\"Y-axis\")\n",
        "ax.axis('on')\n",
        "\n",
        "# Create a custom colorbar\n",
        "cmap = mcolors.ListedColormap(class_colors)\n",
        "bounds = np.arange(13) - 0.5  # Create boundaries for each color\n",
        "norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "cbar = fig.colorbar(plt.cm.ScalarMappable(cmap=cmap, norm=norm),\n",
        "                    ax=ax,\n",
        "                    ticks=np.arange(num_classes),\n",
        "                    spacing='proportional',\n",
        "                    label='Class Number',\n",
        "                    shrink=0.6)\n",
        "\n",
        "cbar.ax.set_yticklabels([f'Class {i}' for i in range(num_classes)])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zIIlT1W8MvXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_xarray_channels_list(data_array, channels_list):\n",
        "    \"\"\"\n",
        "    Plots xarray DataArrays with height and time dimensions for each channel\n",
        "    specified in a list, displaying 3 plots per row.\n",
        "\n",
        "    Args:\n",
        "        data_array: The xarray DataArray containing the data.\n",
        "        channels_list: A list of channel names to plot.\n",
        "    \"\"\"\n",
        "    cols = 3\n",
        "    num_channels = len(channels_list)\n",
        "    num_rows = 3\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, min(cols, num_channels), figsize=(20, 10))\n",
        "\n",
        "    if num_rows == 1:\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    data_array = data_array.copy()\n",
        "    for i, channel in enumerate(channels_list):\n",
        "        row = i // cols\n",
        "        col = i % cols\n",
        "\n",
        "        if num_channels == 1:\n",
        "          ax = axes[0]\n",
        "        else:\n",
        "          ax = axes[row, col]\n",
        "        channel_data = data_array[..., channel]\n",
        "        image = ax.imshow(channel_data.T, origin='lower', cmap='viridis', vmin=np.percentile(channel_data, [10]), vmax=np.percentile(channel_data, [90]), aspect='equal')\n",
        "        ax.set_title(_HEADS[i])\n",
        "        ax.set_xlabel(\"X-axis\")\n",
        "        ax.set_ylabel(\"Y-axis\")\n",
        "        ax.axis('on')\n",
        "        plt.colorbar(image, label='Pixel Value', shrink=1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "temp_ds = x_valid[2,:,:,0:10]\n",
        "plot_xarray_channels_list(temp_ds, range(0,9))\n"
      ],
      "metadata": {
        "id": "l5Tgx-_2RonR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate class weights\n",
        "if 'y_train' not in locals():\n",
        "    raise NameError(\"y_train NumPy array must be loaded before calculating class weights.\")\n",
        "\n",
        "num_classes = y_train.shape[-1] # Should be 12\n",
        "print(f\"Calculating class weights for {num_classes} classes using the full y_train NumPy array...\")\n",
        "\n",
        "\n",
        "class_pixel_counts = np.sum(y_train, axis=(0, 1, 2))\n",
        "\n",
        "print(f\"Total pixel counts per class: {class_pixel_counts}\")\n",
        "print(f\"Shape of counts: {class_pixel_counts.shape}\") # Should be (12,)\n",
        "\n",
        "# IMPORTANT: Add a small epsilon to prevent division by zero for classes potentially absent in the dataset\n",
        "epsilon = np.finfo(float).eps # A very small positive float number\n",
        "class_weights = 1.0 / (class_pixel_counts + epsilon)\n",
        "\n",
        "class_weights = class_weights * 200.0\n",
        "\n",
        "# Scaling weights to have a mean of 1:\n",
        "class_weights = class_weights / np.mean(class_weights)\n",
        "# print(f\"Mean-normalized class weights: {class_weights}\")\n",
        "\n",
        "class_weights[0] = 0 # We don't care about class 0 (unknown)\n",
        "\n",
        "\n",
        "print(f\"\\nCalculated class weights (inverse frequency * 200): {class_weights}\")\n",
        "print(f\"Class weights shape: {class_weights.shape}\") # Should be (12,)\n",
        "print(f\"Min weight: {np.min(class_weights)}, Max weight: {np.max(class_weights)}\")"
      ],
      "metadata": {
        "id": "DpqfQgtJPFOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare patches"
      ],
      "metadata": {
        "id": "vwgT6si4dGPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16 # Example: Choose a value like 2, 4, 8, 16 based on GPU VRAM\n",
        "SHUFFLE_BUFFER_SIZE_TRAIN = len(x_train) # Shuffle buffer size, often set to the dataset size\n",
        "SHUFFLE_BUFFER_SIZE_VALID = len(x_valid)"
      ],
      "metadata": {
        "id": "sV2WzpL2dQp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(x_data, y_data):\n",
        "    \"\"\"Yields one sample at a time.\"\"\"\n",
        "    num_samples = x_data.shape[0]\n",
        "    for i in range(num_samples):\n",
        "        # Ensure data types are correct for yielding\n",
        "        yield x_data[i].astype(np.float32), y_data[i].astype(np.float32)\n",
        "\n",
        "# Define the shape and type of *one* sample (output of the generator)\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(_TIMES, _HEIGHTS, _N_FEATURES*2), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(_TIMES, _HEIGHTS, num_classes), dtype=tf.float32)\n",
        ")\n",
        "\n",
        "print(\"Creating datasets using from_generator...\")"
      ],
      "metadata": {
        "id": "C_cIm_zOsjdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Dataset\n",
        "train_dataset_gen = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(x_train, y_train), # Use lambda to pass args\n",
        "    output_signature=output_signature\n",
        ")\n",
        "# Shuffle needs care with generators - apply *before* batching\n",
        "# Use reshuffle_each_iteration=True for better shuffling across epochs\n",
        "train_dataset_gen = train_dataset_gen.shuffle(SHUFFLE_BUFFER_SIZE_TRAIN, reshuffle_each_iteration=True)\n",
        "train_dataset_gen = train_dataset_gen.batch(BATCH_SIZE)\n",
        "train_dataset_gen = train_dataset_gen.repeat()\n",
        "train_dataset_gen = train_dataset_gen.prefetch(tf.data.AUTOTUNE)\n",
        "print(\"Train dataset created from generator.\")"
      ],
      "metadata": {
        "id": "PImJT791dgDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation Dataset\n",
        "valid_dataset_gen = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(x_valid, y_valid),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "valid_dataset_gen = valid_dataset_gen.batch(BATCH_SIZE)\n",
        "train_dataset_gen = train_dataset_gen.repeat()\n",
        "valid_dataset_gen = valid_dataset_gen.prefetch(tf.data.AUTOTUNE)\n",
        "print(\"Validation dataset created from generator.\")"
      ],
      "metadata": {
        "id": "EJJKWZSQdtQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Dataset (No shuffling)\n",
        "test_dataset_gen = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(x_test, y_test),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "test_dataset_gen = test_dataset_gen.batch(BATCH_SIZE)\n",
        "test_dataset_gen = test_dataset_gen.prefetch(tf.data.AUTOTUNE)\n",
        "print(\"Test dataset created from generator.\")"
      ],
      "metadata": {
        "id": "TcacrELqduG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nChecking one batch from generator dataset:\")\n",
        "for batch in train_dataset_gen.take(1):\n",
        "    print(\"Input batch shape:\", batch[0].shape)\n",
        "    print(\"Output batch shape:\", batch[1].shape)"
      ],
      "metadata": {
        "id": "oBfPrVxbud9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming valid_dataset_gen is your tf.data.Dataset for validation\n",
        "print(\"Fetching a few batches from validation dataset to check diversity...\")\n",
        "fetched_info = []\n",
        "num_to_check = 5 # Check first 5 batches\n",
        "\n",
        "# Check cardinality (number of batches) if possible\n",
        "val_cardinality = tf.data.experimental.cardinality(valid_dataset_gen)\n",
        "num_batches_total = val_cardinality.numpy() if val_cardinality!=tf.data.experimental.UNKNOWN_CARDINALITY else -1\n",
        "print(f\"Validation dataset cardinality (num batches): {num_batches_total if num_batches_total >= 0 else 'Unknown'}\")\n",
        "if num_batches_total > 0 and num_batches_total < num_to_check:\n",
        "     num_to_check = num_batches_total # Don't try to check more batches than exist\n",
        "\n",
        "if num_batches_total == 0:\n",
        "     print(\"!!! Warning: Validation dataset appears to have zero batches!\")\n",
        "else:\n",
        "    try:\n",
        "        for i, batch in enumerate(valid_dataset_gen.take(num_to_check)):\n",
        "            x_batch, y_batch = batch\n",
        "            info = (x_batch.shape, y_batch.shape, tf.reduce_mean(x_batch).numpy(), tf.reduce_mean(tf.cast(y_batch, tf.float32)).numpy())\n",
        "            print(f\"  Batch {i}: X shape {info[0]}, Y shape {info[1]}, X mean {info[2]:.4f}, Y mean {info[3]:.10f}\")\n",
        "            fetched_info.append(info[2:]) # Store mean/sum for comparison\n",
        "\n",
        "            if np.all(tf.cast(y_batch, tf.float32)[0].numpy() == tf.cast(y_batch, tf.float32)[3].numpy()):\n",
        "              print('!!! Warning: Samples within batch are identical')\n",
        "        # Check if the fetched stats were all identical (simplistic check)\n",
        "        if len(fetched_info) > 1 and len(set(fetched_info)) == 1:\n",
        "             print(\"!!! Warning: Multiple validation batches fetched seem identical! Check generator logic.\")\n",
        "        elif len(fetched_info) > 0:\n",
        "             print(\"Validation batches seem diverse (based on mean/sum).\")\n",
        "        else:\n",
        "             print(\"Could not fetch validation batches to check diversity.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! Error occurred while fetching validation batches: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "-Tg2rKMQKYpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unet"
      ],
      "metadata": {
        "id": "4di-kX3NHFo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "class DisplayAndSaveCallback(Callback):\n",
        "    def __init__(self, epochs_num_to_print = 2):\n",
        "        self.epochs_num_to_print = epochs_num_to_print\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch%self.epochs_num_to_print==0:\n",
        "            # print predicted test results:\n",
        "            y_pred_1 = model.predict(x_test)\n",
        "            y_pred_onehot_1 = tf.one_hot(tf.argmax(y_pred_1[:,:,:,:], axis=3), y_pred_1[:,:,:,:].shape[3])\n",
        "            print(classification_report(y_test.reshape([-1,12]), y_pred_onehot_1.reshape([-1,12])))\n",
        "            # save current model:\n",
        "            model_save_path = f'{model_name}/{str(epoch)}'\n",
        "            model.save(model_save_path)\n",
        "            print(\"model saved:\", model_save_path)\n",
        "\n",
        "\n",
        "class DisplayAndSaveCallback_2(Callback):\n",
        "    def __init__(self, epochs_num_to_print = 2, model_name='default_name', n_classes=12):\n",
        "        self.epochs_num_to_print = epochs_num_to_print\n",
        "        self.model_name = model_name\n",
        "        self.n_classes = 12\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch%self.epochs_num_to_print==0:\n",
        "            # print predicted test results:\n",
        "            y_pred_1 = model.predict(x_test)\n",
        "            y_pred_onehot_1 = tf.one_hot(tf.argmax(y_pred_1[:,:,:,:], axis=3), y_pred_1[:,:,:,:].shape[3])\n",
        "            print(classification_report(y_test.reshape([-1,12]), y_pred_onehot_1.reshape([-1,12])))\n",
        "            # save current model:\n",
        "            model_save_path = self.model_name+str(epoch)+'.keras'\n",
        "            model.save(model_save_path)\n",
        "            print(\"model saved:\", model_save_path)"
      ],
      "metadata": {
        "id": "ObCiubr3HK8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Ensure necessary layers are imported\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate, Rescaling, Lambda, Cropping2D, BatchNormalization, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_unet(img_shape, n_classes=12):\n",
        "    # input layer shape is equal to patch image size\n",
        "    inputs = Input(shape=img_shape)\n",
        "\n",
        "    # rescale images from (0, 255) to (0, 1)\n",
        "    previous_block_activation = inputs  # Set aside residual\n",
        "    encoder_filters = [64, 128, 256, 512]\n",
        "\n",
        "    contraction = {}\n",
        "    # Contraction path: Blocks 1 through 5 are identical apart from the feature depth\n",
        "    for f in encoder_filters:\n",
        "        # First convolution\n",
        "        f_in = previous_block_activation.shape[-1] if previous_block_activation.shape[-1] is not None else f # Approx\n",
        "        x = tf.keras.layers.Conv2D(f, (3, 3), kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Activation('relu')(x)\n",
        "        x = tf.keras.layers.Dropout(0.1)(x)\n",
        "\n",
        "        # Second convolution\n",
        "        x = tf.keras.layers.Conv2D(f, (3, 3), kernel_initializer='he_normal', padding='same')(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "        contraction[f'conv{f}'] = x\n",
        "        x_pool = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "        previous_block_activation = x_pool\n",
        "\n",
        "    # Bottleneck layer\n",
        "    # First convolution\n",
        "    c5 = tf.keras.layers.Conv2D(encoder_filters[-1]*2, (3, 3), kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
        "    c5 = tf.keras.layers.BatchNormalization()(c5)\n",
        "    c5 = tf.keras.layers.Activation('relu')(c5)\n",
        "    c5 = tf.keras.layers.Dropout(0.2)(c5)\n",
        "\n",
        "    # Second convolution\n",
        "    c5 = tf.keras.layers.Conv2D(encoder_filters[-1]*2, (3, 3), kernel_initializer='he_normal', padding='same')(c5)\n",
        "    c5 = tf.keras.layers.BatchNormalization()(c5)\n",
        "    c5 = tf.keras.layers.Activation('relu')(c5)\n",
        "    previous_block_activation = c5\n",
        "\n",
        "    # Expansive path: Second half of the network: upsampling inputs\n",
        "    for f in reversed(encoder_filters): # e.g., [256, 128, 64, 32]\n",
        "        x_upsampled = Conv2DTranspose(f, (2, 2), strides=(2, 2), padding='same')(previous_block_activation)\n",
        "        x_upsampled = BatchNormalization()(x_upsampled)\n",
        "        x_upsampled = tf.keras.layers.Activation('relu')(x_upsampled)\n",
        "        skip_connection = contraction[f'conv{f}']\n",
        "\n",
        "        # Cropping\n",
        "        upsampled_h, upsampled_w = x_upsampled.shape[1], x_upsampled.shape[2]\n",
        "        skip_h, skip_w = skip_connection.shape[1], skip_connection.shape[2]\n",
        "\n",
        "        # Calculate how much the upsampled tensor is larger than the skip tensor\n",
        "        crop_h = max(0, upsampled_h - skip_h)\n",
        "        crop_w = max(0, upsampled_w - skip_w)\n",
        "\n",
        "        if crop_h > 0 or crop_w > 0:\n",
        "            x_upsampled = Cropping2D(cropping=(\n",
        "                (crop_h // 2, crop_h - crop_h // 2), # Top, Bottom crop\n",
        "                (crop_w // 2, crop_w - crop_w // 2)  # Left, Right crop\n",
        "            ), name=f'crop_upsampled_to_skip_{f}')(x_upsampled)\n",
        "\n",
        "        # END cropping\n",
        "        x = concatenate([x_upsampled, skip_connection])\n",
        "        x = Conv2D(f, (3, 3), kernel_initializer='he_normal', padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Conv2D(f, (3, 3), kernel_initializer='he_normal', padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        previous_block_activation = x\n",
        "\n",
        "    outputs = tf.keras.layers.Conv2D(filters=n_classes, kernel_size=(1, 1), activation=\"softmax\")(previous_block_activation)\n",
        "    return Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "ga1blBTgLDDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build model\n",
        "img_height = _HEIGHTS\n",
        "img_width = _TIMES\n",
        "img_channels = _N_FEATURES*2\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print(f'Number of devices: {strategy.num_replicas_in_sync}') # Should print 4"
      ],
      "metadata": {
        "id": "I-ef1t03AigN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_index(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)"
      ],
      "metadata": {
        "id": "EeZPrjYpoqQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiclass_weighted_squared_dice_loss(class_weights: Union[list, np.ndarray, tf.Tensor]):\n",
        "    \"\"\"\n",
        "    Weighted squared Dice loss.\n",
        "    Used as loss function for multi-class image segmentation with one-hot encoded masks.\n",
        "    :param class_weights: Class weight coefficients (Union[list, np.ndarray, tf.Tensor], len=<N_CLASSES>)\n",
        "    :return: Weighted squared Dice loss function (Callable[[tf.Tensor, tf.Tensor], tf.Tensor])\n",
        "    \"\"\"\n",
        "    if not isinstance(class_weights, tf.Tensor):\n",
        "        class_weights = tf.constant(class_weights)\n",
        "\n",
        "    def loss(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Compute weighted squared Dice loss.\n",
        "        :param y_true: True masks (tf.Tensor, shape=(<BATCH_SIZE>, <IMAGE_HEIGHT>, <IMAGE_WIDTH>, <N_CLASSES>))\n",
        "        :param y_pred: Predicted masks (tf.Tensor, shape=(<BATCH_SIZE>, <IMAGE_HEIGHT>, <IMAGE_WIDTH>, <N_CLASSES>))\n",
        "        :return: Weighted squared Dice loss (tf.Tensor, shape=(None,))\n",
        "        \"\"\"\n",
        "        axis_to_reduce = range(1, K.ndim(y_pred))  # Reduce all axis but first (batch)\n",
        "        numerator = y_true * y_pred * class_weights  # Broadcasting\n",
        "        numerator = 2. * K.sum(numerator, axis=axis_to_reduce)\n",
        "\n",
        "        denominator = (y_true**2 + y_pred**2) * class_weights  # Broadcasting\n",
        "        denominator = K.sum(denominator, axis=axis_to_reduce)\n",
        "\n",
        "        return 1 - numerator / denominator\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "1KTDRvPdo6bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your class groups (make these accessible where you define the loss)\n",
        "AEROSOL_INDICES = tf.constant([3, 4, 5, 6], dtype=tf.int32)\n",
        "CLOUD_INDICES = tf.constant([7, 8, 9, 10, 11], dtype=tf.int32)\n",
        "\n",
        "def Dice_plus_GroupConfusion_Loss(\n",
        "  class_weights: Union[List, np.ndarray, tf.Tensor],\n",
        "  group_penalty_factor: float = 0.5, # Hyperparameter to weigh the group penalty\n",
        "  epsilon: float = 1e-6 # Small constant to prevent division by zero\n",
        ") -> Callable[[tf.Tensor, tf.Tensor], tf.Tensor]:\n",
        "  \"\"\"\n",
        "  Combined loss: Weighted squared Dice loss + Group Confusion Penalty.\n",
        "  Penalizes misclassifications between aerosol and cloud groups.\n",
        "\n",
        "  :param class_weights: Class weight coefficients for Dice loss.\n",
        "  :param group_penalty_factor: Weighting factor for the group confusion penalty.\n",
        "  :param epsilon: Small constant for numerical stability.\n",
        "  :return: Combined loss function.\n",
        "  \"\"\"\n",
        "  if not isinstance(class_weights, tf.Tensor):\n",
        "    class_weights = tf.constant(class_weights, dtype=tf.float32)\n",
        "\n",
        "  def loss(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"\n",
        "    Compute combined loss.\n",
        "    :param y_true: True masks (tf.Tensor, shape=(<BATCH_SIZE>, <H>, <W>, <N_CLASSES>), one-hot)\n",
        "    :param y_pred: Predicted masks (tf.Tensor, shape=(<BATCH_SIZE>, <H>, <W>, <N_CLASSES>), softmax probabilities)\n",
        "    :return: Combined loss (tf.Tensor, shape=(None,))\n",
        "    \"\"\"\n",
        "\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "\n",
        "    # --- Dice loss component ---\n",
        "    axis_to_reduce = tuple(range(1, K.ndim(y_pred)))  # Reduce spatial and class axes (H, W, C)\n",
        "    # Weights are broadcasted across batch, H, W\n",
        "    numerator = y_true * y_pred * class_weights\n",
        "    numerator = 2. * K.sum(numerator, axis=axis_to_reduce)\n",
        "\n",
        "    # Broadcasting class_weights\n",
        "    denominator = (y_true + y_pred**2) * class_weights # Using y_true instead of y_true**2\n",
        "    denominator = K.sum(denominator, axis=axis_to_reduce)\n",
        "\n",
        "    dice_loss = 1.0 - (numerator + epsilon) / (denominator + epsilon) # Epsilon added for stability\n",
        "\n",
        "    # --- Group Confusion Penalty component ---\n",
        "    # Get probabilities predicted for aerosol and cloud groups for each pixel\n",
        "    y_pred_aerosol_probs = K.sum(tf.gather(y_pred, AEROSOL_INDICES, axis=-1), axis=-1)\n",
        "    y_pred_cloud_probs = K.sum(tf.gather(y_pred, CLOUD_INDICES, axis=-1), axis=-1)\n",
        "\n",
        "    # Identify if the true class belongs to aerosol or cloud group for each pixel\n",
        "    y_true_is_aerosol = K.sum(tf.gather(y_true, AEROSOL_INDICES, axis=-1), axis=-1)\n",
        "    y_true_is_cloud = K.sum(tf.gather(y_true, CLOUD_INDICES, axis=-1), axis=-1)\n",
        "\n",
        "    # Calculate pixel-wise penalty:\n",
        "    pixel_group_penalty = (y_true_is_aerosol * y_pred_cloud_probs) + (y_true_is_cloud * y_pred_aerosol_probs)\n",
        "\n",
        "    # Average the pixel-wise penalty across spatial dimensions (H, W) to get a per-sample penalty\n",
        "    sample_group_penalty = K.mean(pixel_group_penalty, axis=tuple(range(1, K.ndim(pixel_group_penalty))))\n",
        "\n",
        "    # --- 3. Combine losses ---\n",
        "    # The dice_loss and sample_group_penalty are now per-sample losses (shape [BATCH_SIZE,])\n",
        "    combined_loss = dice_loss + (group_penalty_factor * sample_group_penalty)\n",
        "\n",
        "    return combined_loss\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "h1f45G3EWERz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aerosol_cloud_confusion(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"\n",
        "  Calculates a metric for confusion between aerosol and cloud groups.\n",
        "  A lower value indicates less confusion.\n",
        "\n",
        "  The metric computes the average probability assigned to the wrong group\n",
        "  when the true class belongs to either aerosols or clouds.\n",
        "\n",
        "  :param y_true: True masks (tf.Tensor, shape=(<BATCH_SIZE>, <H>, <W>, <N_CLASSES>), one-hot encoded).\n",
        "  :param y_pred: Predicted masks (tf.Tensor, shape=(<BATCH_SIZE>, <H>, <W>, <N_CLASSES>), softmax probabilities).\n",
        "  :return: Per-sample group confusion score (tf.Tensor, shape=(<BATCH_SIZE>,)).\n",
        "            Keras will average this over the batch and then over the epoch.\n",
        "  \"\"\"\n",
        "  y_pred = tf.cast(y_pred, tf.float32)\n",
        "  y_true = tf.cast(y_true, tf.float32)\n",
        "\n",
        "  # Sum predicted probabilities for aerosol and cloud groups for each pixel\n",
        "  y_pred_aerosol_probs = K.sum(tf.gather(y_pred, AEROSOL_INDICES, axis=-1), axis=-1)\n",
        "  y_pred_cloud_probs = K.sum(tf.gather(y_pred, CLOUD_INDICES, axis=-1), axis=-1)\n",
        "\n",
        "  # Identify if the true class belongs to aerosol or cloud group for each pixel\n",
        "  y_true_is_aerosol = K.sum(tf.gather(y_true, AEROSOL_INDICES, axis=-1), axis=-1)\n",
        "  y_true_is_cloud = K.sum(tf.gather(y_true, CLOUD_INDICES, axis=-1), axis=-1)\n",
        "\n",
        "  # Calculate pixel-wise confusion:\n",
        "  pixel_group_confusion = (y_true_is_aerosol * y_pred_cloud_probs) + \\\n",
        "                          (y_true_is_cloud * y_pred_aerosol_probs)\n",
        "\n",
        "  # Average the pixel-wise confusion across dimensions to get a per-sample confusion score.\n",
        "  per_sample_confusion_score = K.mean(pixel_group_confusion, axis=tuple(range(1, K.ndim(pixel_group_confusion))))\n",
        "\n",
        "  return per_sample_confusion_score"
      ],
      "metadata": {
        "id": "BgX2_fl8r2AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "zZOmylGTpFp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train:\n",
        "  import tensorflow.python.keras.backend as Kr\n",
        "  %load_ext tensorboard\n",
        "  # PARAMETERS\n",
        "  group_penalty_factor = 1"
      ],
      "metadata": {
        "id": "4GTby-ENYyDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if train:\n",
        "  print(model_name)\n",
        "\n",
        "  if not os.path.exists(model_run_name):\n",
        "      try:\n",
        "          os.makedirs(model_run_name)  # Use makedirs for nested directories\n",
        "          print(f\"Directory '{model_run_name}' created successfully.\")\n",
        "      except OSError as e:\n",
        "          print(f\"Error creating directory '{model_run_name}': {e}\")\n",
        "  else:\n",
        "      print(f\"Directory '{model_run_name}' already exists.\")"
      ],
      "metadata": {
        "id": "GM0UcJfcjVWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if train:\n",
        "  %tensorboard --logdir logs/{model_name}"
      ],
      "metadata": {
        "id": "ZzxBKRVGjYE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if train:\n",
        "  with strategy.scope():\n",
        "    model = build_unet(img_shape=(img_width, img_height, img_channels), n_classes = num_classes)\n",
        "    print(model.summary())\n",
        "    model_checkpoint_filepath = '/Saved_Model.keras'\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(model_checkpoint_filepath, monitor=\"val_accuracy\", verbose=1, save_best_only=True, mode=\"max\")\n",
        "\n",
        "    # stop model training early if validation loss doesn't continue to decrease over 10 iterations\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, verbose=1, mode=\"min\", restore_best_weights=True, start_from_epoch=50)\n",
        "\n",
        "    # log training console output to csv\n",
        "\n",
        "    csv_logger = tf.keras.callbacks.CSVLogger('csv_logger', separator=\",\", append=False)\n",
        "\n",
        "    # create list of callbacks\n",
        "    log_dir = f\"logs/{model_name}/fit/\"\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "    epochs_num_to_print_pred = 20  # print reconstruction every n epochs\n",
        "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                                  patience=10, min_lr=5e-7, verbose=1)\n",
        "    callbacks_list = [checkpoint, csv_logger, tensorboard_callback,\n",
        "                      DisplayAndSaveCallback_2(epochs_num_to_print_pred, f'{model_run_name}/model_'),\n",
        "                      reduce_lr, early_stopping]  # early_stopping\n",
        "\n",
        "    # steps\n",
        "    steps_per_epoch = len(x_train) // BATCH_SIZE\n",
        "    if len(x_train) % BATCH_SIZE != 0: # If there's a remainder\n",
        "        steps_per_epoch +=1\n",
        "    print(f'steps per epoch = {steps_per_epoch}')\n",
        "    # compile model\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=5e-3)\n",
        "    model.compile(optimizer=optimizer,loss=Dice_plus_GroupConfusion_Loss(class_weights, group_penalty_factor),metrics=[jaccard_index, aerosol_cloud_confusion])\n",
        "    # Training phase\n",
        "    history = model.fit(train_dataset_gen, epochs=500, validation_data=valid_dataset_gen,\n",
        "                        callbacks=callbacks_list, verbose=1, validation_steps=4,\n",
        "                        steps_per_epoch=steps_per_epoch)\n",
        "\n",
        "    # Save model\n",
        "    model_save_path = f'{model_run_name}/unet_last_epoch.keras'\n",
        "    model.save(model_save_path)\n",
        "    print(\"model saved:\", model_save_path)\n"
      ],
      "metadata": {
        "id": "gHJ3SBr5pE0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if train:\n",
        "  # classification validation report\n",
        "  with strategy.scope():\n",
        "    y_pred = model.predict(valid_dataset_gen)\n",
        "    y_pred_onehot = tf.one_hot(tf.argmax(y_pred[:,:,:,:], axis=3), y_pred[:,:,:,:].shape[3])\n",
        "    print('Classification report for validation dataset on model')\n",
        "    print(classification_report(y_valid.reshape([-1,12]), y_pred_onehot.reshape([-1,12])))"
      ],
      "metadata": {
        "id": "GenlHw6EkCoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Analysis"
      ],
      "metadata": {
        "id": "0NNSxeU27Mp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import model"
      ],
      "metadata": {
        "id": "5gcQpQDtgFKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels1=['0-No Class', '1-Clean atmosphere', '2-Non-typed particles/low conc',\n",
        "         '3-Aerosol: small', '4-Aerosol: large,spherical','5-Aerosol: mixture, partly non-spherical',\n",
        "         '6-Aerosol: large, non-spherical', '7-Cloud: non-typed', '8-Cloud: water droplets',\n",
        "         '9-Cloud: likely water droplets', '10-Cloud: ice crystals', '11-Cloud: likely ice crystals']"
      ],
      "metadata": {
        "id": "BvEoTHT7VzH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "model = keras.saving.load_model(\n",
        "    f'{model_run_name}/unet_last_epoch.keras',\n",
        "    custom_objects={\n",
        "        'loss':Dice_plus_GroupConfusion_Loss,\n",
        "        'jaccard_index':jaccard_index,\n",
        "        'aerosol_cloud_confusion':aerosol_cloud_confusion}, compile=True, safe_mode=False)"
      ],
      "metadata": {
        "id": "at3nJdEwL1zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "get_model_run = files_glob = tf.io.gfile.glob('logs/' + model_run_name + \"*\")\n",
        "latest_run = np.sort(get_model_run)[-1]\n",
        "%tensorboard --logdir logs/{'4_layer_large_filter_no_class0_mean1_group_and_dice_penalty1_data_globalnorm_20250522-133149'} --port=6010"
      ],
      "metadata": {
        "id": "XZNXZJ5iKa53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tbparse import SummaryReader\n",
        "\n",
        "# Specify the path to your TensorBoard log directory\n",
        "log_dir = 'logs/4_layer_large_filter_no_class0_mean1_group_and_dice_penalty1_data_globalnorm_20250522-133149'\n",
        "\n",
        "# Read the scalar data from the log directory\n",
        "reader = SummaryReader(log_dir)"
      ],
      "metadata": {
        "id": "_FJydwVJK8mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = 'logs/4_layer_large_filter_no_class0_mean1_group_and_dice_penalty1_data_globalnorm_20250522-133149'\n",
        "reader = SummaryReader(log_dir)\n",
        "df_tensors = reader.tensors\n",
        "\n",
        "TARGET_TAG = 'epoch_loss'\n",
        "df_filtered = df_tensors[df_tensors['tag'] == TARGET_TAG].sort_values('step').reset_index(drop=True)\n",
        "\n",
        "df_set1 = df_filtered.iloc[0::2].copy()\n",
        "df_set2 = df_filtered.iloc[1::2].copy()\n",
        "\n",
        "smoothing_weight = 0.3\n",
        "\n",
        "df_set1['smoothed_value'] = df_set1['value'].ewm(alpha=smoothing_weight).mean()\n",
        "df_set2['smoothed_value'] = df_set2['value'].ewm(alpha=smoothing_weight).mean()\n",
        "\n",
        "target_step = 146\n",
        "val1_at_step = df_set1.loc[df_set1['step'] == target_step, 'value'].iloc[0]\n",
        "val2_at_step = df_set2.loc[df_set2['step'] == target_step, 'value'].iloc[0]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "\n",
        "plt.plot(df_set1['step'], df_set1['smoothed_value'], label='Training', color='#01153E')\n",
        "plt.plot(df_set2['step'], df_set2['smoothed_value'], label='Validation', color='#40E0D0')\n",
        "\n",
        "plt.axvline(x=target_step, color='red', linestyle=':', linewidth=2, label=f'Step {target_step}')\n",
        "\n",
        "plt.ylim(top=0.5)\n",
        "plt.xlabel('Step')\n",
        "plt.legend(['Training','Validation'],fontsize=8)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-SxiacWaEkUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TARGET_TAG = 'epoch_aerosol_cloud_confusion'\n",
        "df_filtered = df_tensors[df_tensors['tag'] == TARGET_TAG].sort_values('step').reset_index(drop=True)\n",
        "\n",
        "df_set1 = df_filtered.iloc[0::2].copy()\n",
        "df_set2 = df_filtered.iloc[1::2].copy()\n",
        "\n",
        "smoothing_weight = 0.4\n",
        "\n",
        "df_set1['smoothed_value'] = df_set1['value'].ewm(alpha=smoothing_weight).mean()\n",
        "df_set2['smoothed_value'] = df_set2['value'].ewm(alpha=smoothing_weight).mean()\n",
        "\n",
        "target_step = 146\n",
        "val1_at_step = df_set1.loc[df_set1['step'] == target_step, 'value'].iloc[0]\n",
        "val2_at_step = df_set2.loc[df_set2['step'] == target_step, 'value'].iloc[0]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "\n",
        "plt.plot(df_set1['step'], df_set1['smoothed_value'], label='Training', color='#01153E')\n",
        "plt.plot(df_set2['step'], df_set2['smoothed_value'], label='Validation', color='#40E0D0')\n",
        "\n",
        "# Add the vertical line at step 146\n",
        "plt.axvline(x=target_step, color='red', linestyle=':', linewidth=2, label=f'Step {target_step}')\n",
        "\n",
        "plt.ylim(top=0.008, bottom=0.001)\n",
        "plt.xlabel('Step')\n",
        "plt.legend(['Training','Validation'],fontsize=8)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GaXVkpr-aSN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TARGET_TAG = 'epoch_jaccard_index'\n",
        "df_filtered = df_tensors[df_tensors['tag'] == TARGET_TAG].sort_values('step').reset_index(drop=True)\n",
        "\n",
        "df_set1 = df_filtered.iloc[0::2].copy()\n",
        "df_set2 = df_filtered.iloc[1::2].copy()\n",
        "\n",
        "smoothing_weight = 0.01\n",
        "\n",
        "df_set1['smoothed_value'] = df_set1['value'].ewm(alpha=smoothing_weight).mean()\n",
        "df_set2['smoothed_value'] = df_set2['value'].ewm(alpha=smoothing_weight).mean()\n",
        "\n",
        "target_step = 146\n",
        "val1_at_step = df_set1.loc[df_set1['step'] == target_step, 'value'].iloc[0]\n",
        "val2_at_step = df_set2.loc[df_set2['step'] == target_step, 'value'].iloc[0]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "\n",
        "plt.plot(df_set1['step'], df_set1['smoothed_value'], label='Training', color='#01153E')\n",
        "plt.plot(df_set2['step'], df_set2['smoothed_value'], label='Validation', color='#40E0D0')\n",
        "\n",
        "# Add the vertical line at step 146\n",
        "plt.axvline(x=target_step, color='red', linestyle=':', linewidth=2, label=f'Step {target_step}')\n",
        "\n",
        "# plt.ylim(top=0.008)\n",
        "plt.xlabel('Step')\n",
        "plt.legend(['Training','Validation'],fontsize=8)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WWl9baMu_y_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(test_dataset_gen)\n",
        "y_pred_onehot = tf.one_hot(tf.argmax(y_pred[:,:,:,:], axis=3), y_pred[:,:,:,:].shape[3])\n",
        "print('Classification mreport test dataset')\n",
        "print(classification_report(y_test.reshape([-1,12]), y_pred_onehot.reshape([-1,12]), target_names=labels1))"
      ],
      "metadata": {
        "id": "S2G6FhaO02B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_TEST = np.argmax(y_test, axis=3)\n",
        "Y_PRED = np.argmax(y_pred_onehot, axis=3)\n",
        "\n",
        "results = confusion_matrix(Y_TEST.reshape([-1,1]), Y_PRED.reshape([-1,1]), normalize='true')"
      ],
      "metadata": {
        "id": "Tfd3jANY38VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(results, labels1):\n",
        "  plt.figure(figsize=[16,8])\n",
        "  ax = sns.heatmap(results*100, annot=True, cmap='Blues')\n",
        "\n",
        "  # Show all ticks and label them with the respective list entries\n",
        "  ax.set_xticks(np.arange(len(labels1))+0.5)\n",
        "  ax.set_yticks(np.arange(len(labels1))+0.5)\n",
        "  ax.set_xticklabels(labels1, rotation=45, ha=\"right\",\n",
        "                      rotation_mode=\"anchor\", fontsize=10)\n",
        "  ax.set_yticklabels(labels1, rotation=0, ha=\"right\",\n",
        "                      rotation_mode=\"anchor\", fontsize=10)\n",
        "\n",
        "  ax.set_title('Confusion Matrix [%]\\n\\n', fontsize=15)\n",
        "  ax.set_xlabel('\\nPredicted Values', fontsize=10)\n",
        "  ax.set_ylabel('Actual Values ', fontsize=10)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plot_confusion_matrix(results, labels1)"
      ],
      "metadata": {
        "id": "30fvIcjfTaRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze results based on height"
      ],
      "metadata": {
        "id": "N-6jowilrgWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean iou score per row (height)\n",
        "mean_iou_per_row = np.zeros(heights_min_dim)\n",
        "non_0_y_test = np.zeros(heights_min_dim)\n",
        "\n",
        "for n in range(heights_min_dim):\n",
        "  y_test_row = Y_TEST[:,:,n].copy()\n",
        "  y_pred_row = Y_PRED[:,:,n].copy()\n",
        "\n",
        "  y_test_non_0_count = np.sum(np.not_equal(y_test_row, 0))\n",
        "  non_0_y_test[n] = y_test_non_0_count\n",
        "\n",
        "  cm_row = confusion_matrix(y_test_row.reshape([-1,1]), y_pred_row.reshape([-1,1]), normalize='true', labels=np.arange(num_classes))\n",
        "  tp = np.diag(cm_row)\n",
        "  fp = np.sum(cm_row, axis=0) - tp\n",
        "  fn = np.sum(cm_row, axis=1) - tp\n",
        "  iou_per_class = tp / (tp + fp + fn + 1e-7)\n",
        "  relevant_iou_scores = iou_per_class[1:] # Exclude class 0\n",
        "  mean_iou_per_row[n] = np.nanmean(relevant_iou_scores)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZGQH-ogmaVMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Plotting ---\n",
        "x_axis_coords = np.arange(heights_min_dim)\n",
        "\n",
        "# Plot: Mean IoU\n",
        "\n",
        "fig = px.scatter(x=x_axis_coords, y=mean_iou_per_row,\n",
        "                 width=1000, height=500,\n",
        "                 color=non_0_y_test,\n",
        "                 color_continuous_scale=px.colors.sequential.Viridis,\n",
        "                 labels={'color': 'Count'}\n",
        "                 )\n",
        "fig.update_yaxes(title_text=\"Jaccard Index\")\n",
        "fig.update_xaxes(title_text=\"Height Index\")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "tko_KxOmqAkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze actual images"
      ],
      "metadata": {
        "id": "u1RrooGEIGR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MidpointNormalize(mpl.colors.Normalize):\n",
        "    def __init__(self, vmin=None, vmax=None, vcenter=None, clip=False):\n",
        "        self.vcenter = vcenter\n",
        "        super().__init__(vmin, vmax, clip)\n",
        "\n",
        "    def __call__(self, value, clip=None):\n",
        "        x, y = [self.vmin, self.vcenter, self.vmax], [0, 0.5, 1.]\n",
        "        return np.ma.masked_array(np.interp(value, x, y,\n",
        "                                            left=-np.inf, right=np.inf))\n",
        "\n",
        "    def inverse(self, value):\n",
        "        y, x = [self.vmin, self.vcenter, self.vmax], [0, 0.5, 1]\n",
        "        return np.interp(value, x, y, left=-np.inf, right=np.inf)"
      ],
      "metadata": {
        "id": "7_5CJOh4Oro1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_colors = [\n",
        "        # Background / Clear Air\n",
        "        '#f0f0f0',  # 0: No Class (Very Light Gray)\n",
        "        '#d9d9d9',  # 1: Clean Atmosphere (Light Gray)\n",
        "\n",
        "        # Non-typed\n",
        "        '#969696',  # 2: Non-typed particles (Medium Gray)\n",
        "\n",
        "        # Aerosol Group (Warm Colors)\n",
        "        '#fee08b',  # 3: Aerosol: small (Bright Yellow)\n",
        "        '#fdae61',  # 4: Aerosol: large, spherical (Strong Orange)\n",
        "        '#f46d43',  # 5: Aerosol: mixture (Red-Orange)\n",
        "        '#d73027',  # 6: Aerosol: large, non-spherical (Strong Red)\n",
        "\n",
        "        # Cloud Group (Cool Colors)\n",
        "        '#66c2a5',  # 7: Cloud: non-typed (Teal)\n",
        "        '#3288bd',  # 8: Cloud: water droplets (Strong Blue)\n",
        "        '#4393c3',  # 9: Cloud: likely water droplets (Slightly Lighter Blue)\n",
        "        '#5e4fa2',  # 10: Cloud: ice crystals (Deep Purple)\n",
        "        '#9e9ac8',  # 11: Cloud: likely ice crystals (Lighter Purple/Lavender)\n",
        "    ]"
      ],
      "metadata": {
        "id": "rCAzoYkYxz3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from scipy.ndimage import gaussian_filter # For heatmap density\n",
        "\n",
        "def plot_segmentation_results(\n",
        "    y_true_all: np.ndarray,\n",
        "    y_pred_all: np.ndarray,\n",
        "    image_index: int,\n",
        "    class_labels: list = range(0, num_classes),\n",
        "    figsize: tuple = (34, 8),\n",
        "    group_confusion_sigma: float = 1.0, # Sigma for Gaussian filter for density heatmap\n",
        "    confusion_sigma: float = 0.5, # Sigma for Gaussian filter for density heatmap\n",
        "    with_suptitle = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots a side-by-side comparison of the ground truth and predicted segmentation maps.\n",
        "\n",
        "    Args:\n",
        "        y_true_all: NumPy array of ground truth integer labels for all samples.\n",
        "                    Shape: (num_samples, image_height, image_width).\n",
        "        y_pred_all: NumPy array of predicted integer class labels for all samples.\n",
        "                    Shape: (num_samples, image_height, image_width).\n",
        "        image_index: The index of the image to plot from the batch of samples.\n",
        "        class_labels: A list of strings with the names for each class index.\n",
        "                      The length should be equal to the number of classes.\n",
        "        figsize: A tuple specifying the figure size for the plot.\n",
        "    \"\"\"\n",
        "    if not (0 <= image_index < y_true_all.shape[0]):\n",
        "        print(f\"Error: image_index {image_index} is out of bounds. Please choose an index between 0 and {y_true_all.shape[0] - 1}.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    # Extract the specific image slices to plot\n",
        "    y_true_image = y_true_all[image_index,...].copy()\n",
        "    y_pred_image = y_pred_all[image_index,...].copy()\n",
        "\n",
        "    group_confusion_error_map = np.zeros_like(y_true_image, dtype=np.uint8)\n",
        "    confusion_error_map = np.zeros_like(y_true_image, dtype=np.uint8)\n",
        "    num_classes = len(class_labels)\n",
        "\n",
        "    # Create boolean masks for aerosol and cloud classes for y_true and y_pred\n",
        "    is_true_aerosol = np.isin(y_true_image, AEROSOL_INDICES)\n",
        "    is_true_cloud = np.isin(y_true_image, CLOUD_INDICES)\n",
        "\n",
        "    is_pred_aerosol = np.isin(y_pred_image, AEROSOL_INDICES)\n",
        "    is_pred_cloud = np.isin(y_pred_image, CLOUD_INDICES)\n",
        "\n",
        "    is_pred_typed = np.isin(y_pred_image, tf.concat([AEROSOL_INDICES, CLOUD_INDICES], 0))\n",
        "    is_true_typed = np.isin(y_true_image, tf.concat([AEROSOL_INDICES, CLOUD_INDICES], 0))\n",
        "    is_typed = np.logical_and(is_pred_typed, is_true_typed)\n",
        "    # Condition 1: True is Aerosol, Predicted is Cloud\n",
        "    true_aero_pred_cloud = np.logical_and(is_true_aerosol, is_pred_cloud)\n",
        "\n",
        "    # Condition 2: True is Cloud, Predicted is Aerosol\n",
        "    true_cloud_pred_aero = np.logical_and(is_true_cloud, is_pred_aerosol)\n",
        "\n",
        "    # Combine conditions: Mark 1 where either confusion type occurs\n",
        "    group_confusion_error_map[np.logical_or(true_aero_pred_cloud, true_cloud_pred_aero)] = 1\n",
        "\n",
        "    # General onfusion\n",
        "    confusion_error_map[np.logical_and(np.not_equal(y_pred_image, y_true_image), is_typed)] = 1\n",
        "\n",
        "    # group confusion density\n",
        "    # Higher sigma = more smoothing, larger \"bright\" areas.\n",
        "    group_confusion_density_map = gaussian_filter(group_confusion_error_map.astype(float), sigma=group_confusion_sigma)\n",
        "    confusion_density_map = gaussian_filter(confusion_error_map.astype(float), sigma=confusion_sigma)\n",
        "\n",
        "    # --- Set up colormaps and normalizations ---\n",
        "    cmap_labels = plt.get_cmap('nipy_spectral', num_classes)\n",
        "    bounds_labels = np.arange(-0.5, num_classes, 1)\n",
        "    norm_labels = mcolors.BoundaryNorm(bounds_labels, cmap_labels.N)\n",
        "\n",
        "    cmap_binary_error = mcolors.ListedColormap(['lightgray', 'darkorange'])\n",
        "    bounds_binary_error = [-0.5, 0.5, 1.5]\n",
        "    norm_binary_error = mcolors.BoundaryNorm(bounds_binary_error, cmap_binary_error.N)\n",
        "\n",
        "    cmap_density_heatmap = plt.get_cmap('hot') # 'hot', 'inferno', 'magma' are good for heatmaps\n",
        "\n",
        "    # --- Set up colormap and normalization for discrete classes ---\n",
        "    # cmap = plt.get_cmap('tab20', num_classes)\n",
        "    cmap = mcolors.ListedColormap(custom_colors)\n",
        "    cmap_error = mcolors.ListedColormap(['black', 'red']) # 0: Match, 1: Mismatch\n",
        "    bounds_error = [-0.5, 0.5, 1.5]\n",
        "    norm_error = mcolors.BoundaryNorm(bounds_error, cmap_error.N)\n",
        "\n",
        "    # Create a normalization object to map integer class values to the colormap.\n",
        "    # Boundaries are set between the integers.\n",
        "    bounds = np.arange(-0.5, num_classes, 1)\n",
        "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "    # --- Plotting ---\n",
        "    fig, axs = plt.subplots(1, 4, figsize=figsize)\n",
        "    if with_suptitle:\n",
        "      fig.suptitle(f\"Segmentation Result for Image #{image_index}\", fontsize=15, y=0.8)\n",
        "\n",
        "    # Plot Group Confusion Error Map\n",
        "    im3 = axs[0].imshow(group_confusion_density_map.T, cmap=cmap_density_heatmap, aspect='equal')\n",
        "    axs[0].set_title(\"Aerosol-Cloud Group Confusion Density Heatmap\")\n",
        "    axs[0].set_xlabel(\"Time (X-axis / Width)\")\n",
        "    axs[0].set_ylabel(\"Height (Y-axis)\")\n",
        "\n",
        "    # Plot general confusion heatmap\n",
        "    im4 = axs[1].imshow(confusion_density_map.T, cmap=cmap_density_heatmap, aspect='equal')\n",
        "    axs[1].set_title(\"Confusion heatmap\")\n",
        "    axs[1].set_xlabel(\"Time (X-axis)\")\n",
        "    axs[1].set_ylabel(\"Height (Y-axis)\")\n",
        "\n",
        "    # Plot Ground Truth\n",
        "    im1 = axs[2].imshow(y_true_image.T, cmap=cmap, norm=norm, aspect='equal')\n",
        "    axs[2].set_title(\"Ground Truth (Test Labels)\")\n",
        "    axs[2].set_xlabel(\"Time (X-axis)\")\n",
        "    axs[2].set_ylabel(\"Height (Y-axis)\")\n",
        "\n",
        "    # Plot Prediction\n",
        "    im2 = axs[3].imshow(y_pred_image.T, cmap=cmap, norm=norm, aspect='equal')\n",
        "    axs[3].set_title(\"Model Prediction\")\n",
        "    axs[3].set_xlabel(\"Time (X-axis)\")\n",
        "    axs[3].set_ylabel(\"Height (Y-axis)\")\n",
        "\n",
        "    # Invert y-axis so that height 0 is at the top\n",
        "    for ax in axs:\n",
        "        ax.invert_yaxis()\n",
        "\n",
        "    # --- Create and configure the colorbar ---\n",
        "    ax_position = axs[1].get_position()\n",
        "\n",
        "    fig.subplots_adjust(right=0.85)\n",
        "\n",
        "    cbar_ax = fig.add_axes([\n",
        "        0.87,\n",
        "        ax_position.y0 + 0.01,\n",
        "        0.02,\n",
        "        ax_position.height - 0.02\n",
        "    ])\n",
        "\n",
        "    cbar = fig.colorbar(im1, cax=cbar_ax)\n",
        "\n",
        "    # Set the ticks to be in the middle of each color segment\n",
        "    tick_locs = np.arange(num_classes)\n",
        "    cbar.set_ticks(tick_locs)\n",
        "\n",
        "    # Set the tick labels to your class labels\n",
        "    cbar.set_ticklabels(class_labels)\n",
        "    cbar.ax.tick_params(labelsize=10)\n",
        "    cbar.set_label(\"Class Labels\", rotation=270, labelpad=15)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_segmentation_results(Y_TEST, Y_PRED, 10)"
      ],
      "metadata": {
        "id": "lth_yWrQrIxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find case studies"
      ],
      "metadata": {
        "id": "NoPjouNnjfdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_iou_per_row = np.zeros(heights_min_dim)\n",
        "non_0_y_test = np.zeros(heights_min_dim)\n",
        "\n",
        "def get_height_analysis_graph(y_test, y_pred, image_num):\n",
        "  for n in range(heights_min_dim):\n",
        "    y_test_row = y_test[image_num,:,n].copy()\n",
        "    y_pred_row = y_pred[image_num,:,n].copy()\n",
        "\n",
        "    y_test_non_0_count = np.sum(np.not_equal(y_test_row, 0))\n",
        "    non_0_y_test[n] = y_test_non_0_count\n",
        "\n",
        "    cm_row = confusion_matrix(y_test_row.reshape([-1,1]), y_pred_row.reshape([-1,1]), normalize='true', labels=np.arange(num_classes))\n",
        "    tp = np.diag(cm_row)\n",
        "    fp = np.sum(cm_row, axis=0) - tp\n",
        "    fn = np.sum(cm_row, axis=1) - tp\n",
        "    iou_per_class = tp / (tp + fp + fn + 1e-7)\n",
        "    relevant_iou_scores = iou_per_class[1:] # Exclude class 0\n",
        "    mean_iou_per_row[n] = np.nanmean(relevant_iou_scores)\n",
        "\n",
        "  # --- Plotting ---\n",
        "  y_axis_coords = np.arange(heights_min_dim)\n",
        "\n",
        "  # Plot: Mean IoU\n",
        "\n",
        "  fig = px.scatter(x=mean_iou_per_row, y=y_axis_coords,\n",
        "                  title='Mean IoU vs. Height (Excl Class 0)',\n",
        "                  width=1000, height=500,\n",
        "                  color=non_0_y_test,\n",
        "                  color_continuous_scale=px.colors.sequential.Viridis,\n",
        "                  labels={'color': 'Count'}\n",
        "                  )\n",
        "  fig.update_xaxes(title_text=\"Jaccard Value (iou)\")\n",
        "  fig.update_yaxes(title_text=\"Height Index\")\n",
        "\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "ggqLlsQEtD4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_num = 36\n",
        "plot_segmentation_results(Y_TEST, Y_PRED, image_num, with_suptitle=False)\n",
        "\n",
        "results_36 = confusion_matrix(Y_TEST[image_num,...].reshape([-1,1]), Y_PRED[image_num,...].reshape([-1,1]), normalize='true')\n",
        "plot_confusion_matrix(results_36, labels1[:7])\n",
        "print(classification_report(y_test[image_num,...].reshape([-1,12])[...,:7], y_pred_onehot[image_num,...].reshape([-1,12])[...,:7], target_names=labels1[:7]))"
      ],
      "metadata": {
        "id": "vh-8KxwajiMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_num = 12\n",
        "plot_segmentation_results(Y_TEST, Y_PRED, image_num, with_suptitle=False)\n",
        "\n",
        "results_12 = confusion_matrix(Y_TEST[image_num,...].reshape([-1,1]), Y_PRED[image_num,...].reshape([-1,1]), normalize='true')\n",
        "plot_confusion_matrix(results_12, labels1)\n",
        "print(classification_report(y_test[image_num,...].reshape([-1,12]), y_pred_onehot[image_num,...].reshape([-1,12]), target_names=labels1))"
      ],
      "metadata": {
        "id": "pU45JXJ6jkrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lidar signal attenuation with thick cloud\n",
        "image_num = 70\n",
        "plot_segmentation_results(Y_TEST, Y_PRED, image_num, with_suptitle=False)\n",
        "results_70 = confusion_matrix(Y_TEST[image_num,...].reshape([-1,1]), Y_PRED[image_num,...].reshape([-1,1]), normalize='true')\n",
        "plot_confusion_matrix(results_70, labels1)\n",
        "print(classification_report(y_test[image_num,...].reshape([-1,12]), y_pred_onehot[image_num,...].reshape([-1,12]), target_names=labels1))"
      ],
      "metadata": {
        "id": "FDI--3s3jk3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mixed phase clouds"
      ],
      "metadata": {
        "id": "4mWAI5vbjw4P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}